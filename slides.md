---
theme: seriph
layout: center
title: Data Science for Community Engaged Research
---

# Data Science for Community Engaged Research

Here's where I'll put my (Jeremy's) info and stuff about the projects generally.

---
layout: two-cols-header
---

# Neha Anil Chede

- What did you accomplish? What were the results, products, code, or other deliverables that you provided?

  - Integration of R and Python: Successfully integrated R and Python to execute R scripts and generate advanced visualizations, like, spiral plot and solar correlation plot with Streamlit components. The final deliverable was a Streamlit app capable of dynamically rendering aplots directly from R code, showcasing how data visualization libraries from both ecosystems can be used in tandem.
  - Implemented a dynamic colour mapping strategy that involved sampling from a large colour palette, ensuring a visually intuitive representation where higher values automatically have more prominence. 
  - Network Simulation: I developed a three-layer network simulation in Streamlit and Python, incorpating discounted layers and mutatuons to create a dynamic network model. 
  - Web Scraping: I explored web scraping techniques and language models through independent research and hands-on practice.

- What did you learn? What skills did you acquire?
  - Gained hands-on experience with complex R visualization libraries, learning to create visually appealing and functionally effective plots which cater to specific data presentation needs.
  - Enhanced my skills in developing interactive applications using Streamlit, focusing on displaying complex visualizations directly from R outputs. This included learning about Streamlit components for displaying HTML and SVG content dynamically.
  - Through addressing and resolving various errors related to library imports and plot rendering, I honed my debugging and problem-solving skills, crucial for developing robust data applications that involve multiple technologies.
  - Neo4j and Network Modeling: I explored the use of Neo4j, gaining a deeper understanding of how graph databases can model complex relationships and store interconnected data. I investigated methods for exporting data from Neo4j into Python, providing a seamless workflow for using graph data in Python-based analysis and simulations.
  - Web Scraping and NLP Techniques: Through independent research and experimentation, I learned the basics of web scraping and natural language processing (NLP) techniques. I practiced extracting data from websites and using language models like WordNet to understand text classification and data augmentation strategies. This exploration helped me understand how to navigate complex website structures and perform basic data extraction tasks, laying the groundwork for future data acquisition projects.

- Any concluding thoughts about your experience?
  - Interdisciplinary Technical Development: This project provided a valuable opportunity to explore the interdisciplinary nature of data science and software development. I applied concepts from data visualization, frontend development, and cross-language integration, enhancing my ability to deliver comprehensive solutions.
  - Real-World Problem Solving: Working through real-world challenges, such as managing cross-language plot rendering and dynamic color assignment, was both challenging and rewarding. It solidified my understanding of how different tools and libraries can be combined to solve complex problems effectively.
  - Versatility and Adaptability: The experience demonstrated my ability to adapt quickly to new libraries and technologies, manage cross-functional tasks, and build solutions that align with both user and technical requirements. This adaptability is a critical asset in the ever-evolving field of technology. It has laid a foundation for taking on more ambitious projects that involve complex data integration and visualization tasks.
  - Future Potential in Web Scraping and NLP: My exploration into web scraping and language models has opened new avenues for future projects. While not directly applied in this project, the knowledge gained has prepared me to handle data extraction and NLP challenges with more confidence and creativity.

::left::

# Kirthivasan Pandurangan Neelavathi

- What did you accomplish? What were the results, products, code, or other deliverables that you provided?
- What did you learn? What skills did you acquire?
- Any concluding thoughts about your experience?

::right::

# Vivek Tiwari

- What did you accomplish? What were the results, products, code, or other deliverables that you provided?
  - For the Center-IMPACT project:
  - I single-handedly developed a comprehensive Streamlit frontend, creating an intuitive interface for project creation, survey forms, and real-time score calculations. This streamlined the entire process of evaluating community-engaged research projects.
  - I architected and implemented a sophisticated Neo4j graph database, modeling intricate relationships between project entities. This complex structure allows for powerful querying and analysis, providing deeper insights into research impact.
  - I crafted a custom API that seamlessly connects the frontend and backend, ensuring smooth data flow and enhancing overall system performance.
  - I took charge of deployment, automating hosting with Streamlit cloud and implementing robust CI/CD pipelines on GitHub. This significantly improved our development workflow and ensured consistent, reliable updates.
  - Throughout the development phase, I managed the hosting environment, providing a stable platform for testing and iteration, which was crucial for the project's success.
  
  - For the Center-SEEK project:
  - I engineered an advanced web scraping script that efficiently collected news articles from multiple university websites, overcoming various technical challenges to gather a diverse dataset.
  - I designed and trained a Feedforward Neural Network that achieved an impressive 71% accuracy in classifying community-engaged research articles, despite the initial limited dataset.
  - To overcome data scarcity, I implemented a suite of complex NLP data augmentation techniques. This included synonym replacement using WordNet, strategic word swapping and deletion, and contextual word insertion using BERT. These techniques dramatically expanded our training data.
  - Through my efforts, I transformed a limited initial dataset of just 92 articles into a robust training set, significantly enhancing the model's learning capabilities and overall performance.

- What did you learn? What skills did you acquire?
  Through these projects, I significantly expanded my expertise:
  - I mastered Streamlit for rapid frontend development, learning to create responsive, data-centric web applications efficiently.
  - I gained advanced skills in Neo4j, developing a deep understanding of graph database design principles and their real-world applications.
  - I honed my web scraping techniques, learning to navigate complex website structures and extract relevant data effectively.
  - I became proficient in cutting-edge NLP data augmentation methods, gaining hands-on experience with tools like WordNet and BERT.
  - I deepened my understanding of neural network architecture, particularly in designing and fine-tuning Feedforward Neural Networks for text classification tasks.
  - I strengthened my DevOps skills through implementing CI/CD pipelines and managing cloud hosting, enhancing my ability to streamline development workflows.
  - I improved my API development skills, learning to create robust interfaces for seamless system integration.
  - I developed innovative strategies for handling small datasets, acquiring valuable skills in data preprocessing and augmentation for machine learning applications.
  - I gained practical experience in applying deep learning techniques to real-world text classification challenges.
  - I enhanced my project management abilities, successfully overseeing end-to-end development from initial data collection to final model deployment.

- Any concluding thoughts about your experience?
  - These projects were transformative for my professional growth. I tackled complex, real-world challenges in community-engaged research, pushing the boundaries of my technical abilities. The experience of overcoming data limitations through creative NLP techniques was particularly enlightening and has equipped me with valuable problem-solving skills. I demonstrated my versatility by seamlessly transitioning between frontend development, backend architecture, machine learning, and DevOps. This interdisciplinary approach allowed me to create comprehensive solutions that bridge technical implementation and domain-specific needs.
  - The projects also honed my ability to work independently and take ownership of critical components. From designing database structures to implementing machine learning models, I drove key aspects of both projects to successful completion.
  Perhaps most importantly, these experiences have instilled in me a confidence to take on ambitious, multifaceted projects. I've proven my ability to quickly adapt to new technologies and methodologies, positioning myself as a versatile and valuable asset for future technological challenges.
  - As I reflect on these projects, I'm excited about the foundation they've provided for my future work. I'm now well-equipped to tackle even more complex challenges, contribute to cutting-edge research, and continue pushing the boundaries of what's possible in community-engaged tech solutions.
